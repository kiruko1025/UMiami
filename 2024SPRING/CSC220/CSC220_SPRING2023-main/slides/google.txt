REVIEW:  INTERFACES, METHODS, and IMPLEMENTATIONS

Interface:  List
Methods:  size(), add(x), add(i, x), remove(i), get(i), set(i, x)
Implementations:  ArrayList, LinkedList

Interface:  Map
Methods:  size(), get(k), put(k, v), containsKey(k), keySet()
Implementations:  TreeMap, HashMap -- SortedPD (Map<String,String>), LinkedMap, SkipMap, BST, BTree, HashTable

Interface:  Deque (a.k.a. StackInterface)
Methods:  empty(), peek(), pop(), push(x)
Implementations:  ArrayDeque, LinkedList -- ArrayStack, LinkedStack

Interface:  Queue
Methods:  size(), offer(x), peek(), poll(), [add(x), element(), remove()]
Implementations:  ArrayDeque, LinkedList, PriorityQueue -- ArrayQueue, LinkedQueue, Heap

Interface:  Comparable
Methods:  compareTo(that)

Interface:  Comparator
Methods:  compare(x, y)

Interface:  Iterator
Methods:  hasNext(), next()

Interface:  Iterable
Methods:  iterator()
Implementations:  everything

Interface: Set (Like a Map without a value.)
Methods:  add(x), contains(x), remove(x)
Implementations: TreeSet, HashSet

Make sure you know that running times of the methods for each implementation.


COMPETING WITH GOOGLE

Let's consider a simple description of the Internet.  The Internet consists of WEB PAGES
which are actually just files.  There are many types of files, but the ones
which interest us are written in HTML (Hyper-Text Mark-up Language).  Each web page is
accessed by its URL (Uniform (or universal) Resource Locator), which looks like this:

	http://www.cs.miami.edu/home/vjm/csc220/index.html

An HTML file has words (text) on it and URLs (links), written in a special way:

	Here is some 
	<A HREF=http://download.oracle.com/javase/8/docs/api/>
	Java Documentation</A>

This makes the words "Here is some Java Documentation" appear, but when you click on
"Java Documentation", it goes to the web page with URL

	http://download.oracle.com/javase/8/docs/api/index.html

By the way, if no file is mentioned, the default is index.html.  If there is no
index.html, it just lists the directory, like every time you go to one of my prog
directories.

When we give some search words to Google, such as

	Victor Milenkovic Java

Google finds all web pages which contain those words and ranks them by significance.
It's a lot more complicated now, but originally the significance was determined by the
the PageRank algorithm.  It's a little bit of MTH210 that's worth a trillion dollars!

The BASIC idea is that if a web page is "good", then people will "vote" for it by
putting links to it in their web pages.  If this were done honestly, then the
original idea would work fine.  Of course, people try to subvert this idea for fun
or profit by creating lots of "dummy" web pages that link to a page they want people
to go to.  It's like stuffing the ballot box in the old days of voting.


INDEXING WEB PAGES

Obviously, Google doesn't do the search by going out on the Internet when you make a
search request.  Instead, they collect information on web pages ahead of time and
store this information on their own servers.  They have to store it in a way that
facilitates rapid search.  This is often called INDEXING.  We now know enough
techniques to have a shot at explaining how Google manages to organize their
information in a way that allows searches in a fraction of a second.  Since we don't
really know how Google does it, we will talk about a search company called NotGPT
and discuss how they might do it.

To start with, indexing is not mysterious at all.  Every time NotGPT sees a new
web page, it creates a FILE on one of its hard disks to store information about the
page, such as its URL, date it was seen, pages it links to, significance, and
perhaps even a compressed copy.

Inside the computer, you address individual BYTES.  On a hard disk, the smallest
unit that can be addressed is a BLOCK, usually 512 (2^9) bytes.  A file and the
web page it represents can be indexed by the address of its first block.  However,
disks are only terabytes in size.  A 8T (2^43 byte) disk has 16 billion (2^34)
blocks (34+9=43).  But there are 100s of billions of web pages out there.  So NotGPT
is going to need at least 100 hard disks, probably many more.  Still, that's pretty
reasonable for a company.

The index includes the disk number followed by the block number on that disk.  Six
bytes is probably plenty.  The first 12 bytes indicate the disk number and the
remaining 34 are the block number on the 8T disk.  In any case, an int is not enough
since it is only four bytes.  An 8-byte long is way more than enough.  (Of course, when
we switched from 16-bit integers to 32-bit integers, we thought those were WAY more
than enough.)

So the index of a web page is a unique long (64-bit) integer.  Since it corresponds to
a disk and a block on a disk, we can get to the web page information file in one disk
seek, about 1ms.


HAVE WE INDEXED A PAGE ALREADY?

Suppose we see a link, http://www.cs.miami.edu/home/vjm/csc220/index.html.  Do we need to
index it?  Or is it indexed already?  How can NotGPT know?

Even if each info file contains the URL, we would have to scan multiple disks to find
the file, if it exists.  That will take a ridiculous amount of time.

We need a Map from the URL to the index of the web page.  If the URL is not a key,
then it has not been indexed yet.  With 100s of billions of web pages, this Map
cannot fit in RAM.  It will barely fit on one hard disk!


B+ TREE MAP FROM URL TO INDEX

A B+ Tree with a large branching factor, like 1024, makes a good external data
structure.  For 100s of billions of URLs, four disk seeks (4 ms) is not so bad.

However, there is a problems with using URLs as keys.


NORMALIZING URLS

What's wrong with

	http://www.cs.miami.edu/home/vjm/csc220/index.html

?  It's like

	April 11, 2022

It goes from month to day then jumps back to year.  And those Europeans shouldn't
feel so smug.  They would say 11/04/2022, which is completely backwards, although at
least it is consistent.  But what if they include a time?  The quiz is at 11:20 on
11/02/2023?  Or 11/02/2023 at 11:20?  Inconsistent again.  Or are we going to say
the quiz starts at 20:11?

So we will use the format

	edu.miami.cs.www/home/vjm/csc220/index.html

We drop the http:// because we will only work with web pages.

This fixes the first problem.  The following two strings are far apart:

	http://www.miami.edu
	http://www.cs.miami.edu

http://www.fiu.edu is closer to http://www.miami.edu than http://www.cs.miami.edu.

But the normalized versions are close:

	edu.miami.www
        edu.miami.cs.www


INDEXING PAGES AGAIN

For our implementation, we will use a InfoFile class to represent the page file.  It
contains the URL, indices of pages it links, and its significance (to be determined
later).

In the final project, a HardDisk class will map a Long page index to its InfoFile,
and a BTree will map a URL to its page index.


INDEXING WORDS

Each word also needs its own file.  The information we need to store about a word is
the list of web pages that contain that word.  For example, in order to answer a
query like "Victor Milenkovic Java", we need to know the web pages which contain
Victor, those which contain Milenkovic, and those which contain Java.  (Then we will
take the intersection of these three lists.)  We represent each page using its
index, which is much shorter than its URL.  To store the list of indices, we will
use a InfoFile class, even though it has an extraneous impact field.

We also need a Map from each word to its index.  Since there are only MILLIONS of
words, this Map can really be stored in RAM.  Furthermore, since adjacent words are
unrelated alphabetically, the Map does not need to be ordered.  In contrast, the
URLs on a page ARE likely to be close alphabetically, and that makes the B+ tree
"virtually O(1) time".  So we will use a HashMap to map each word to its WordFile
index.


COLLECTING ALL WEB PAGES

GoDaddy is a domain name registrar.  So when my wife wanted to create
sleuthacademy.org, she paid GoDaddy some money and there it was.  (She also has to
host it somewhere.)  People register about one new domain PER SECOND, most with
GoDaddy.

Each domain has multiple web pages, and Google wants to index them all.  So every day,
GoDaddy tells Google about the new domains and Google indexes all their web pages and
everything they link to.

How can we do the same thing?  We get a bunch of URLs.  For each one, check to make
sure we haven't seen it before.  If not, index it and put its index in a Queue.

While the Queue is not empty, take out an index.  Using the Browser I provide you,
get the List of words and URLs on that page.  For each new URL, index it and put it
in the queue.

This is the same BREADTH FIRST SEARCH we used to get from snow to rain (and all
other words).


AVOID DUPLICATION

The Browser class allows you to load a page, given its URL, and get a list of URLs
and a list of words on that page.

For each URL, if it has not been indexed, index it and add it to the queue.  Add its
index to the InfoFile of the current page.

If the same URL appears multiple times on a page, does it get added multiple times?
Suppose I put sleuthacademy.org a thousand times on my home page.  Should that mean
more than just putting it once?  Page and Brin thought not.  What do you think?

Question: As you visit each URL on that page, how can you tell if you have seen it
before on that page?  Answer: store the URLs you have seen in a Set<String>.  If you
see a URL a second, third, or more, time, you know you don't have to look up its
index, index it, enqueue, nor add its index to the file.

For each word on the page, if that is the first time it appears on the page, check
if it needs to be indexed.  Add the index of the current page to the word's WordFile.
Of course, you can ignore a word if you have already seen it before on this page.

mary.txt shows the result of indexing a small web site.  This will be the output of
your lab and homework this week.


